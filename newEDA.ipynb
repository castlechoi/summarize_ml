{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbAQDAC4cZGLhDgTblqZeK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###pre"
      ],
      "metadata": {
        "id": "fiwK03v1P3tJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQWtFd32OLUA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier,RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')"
      ],
      "metadata": {
        "id": "sPTuTNhnOseO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df.drop(columns = ['TIMESTAMP','PRODUCT_ID', 'Y_Quality','Y_Class'])\n",
        "train_y = train_df[['Y_Quality','LINE']]\n",
        "test_x = test_df.drop(columns = ['TIMESTAMP','PRODUCT_ID'])"
      ],
      "metadata": {
        "id": "GA8er46iO1oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_lst = ['A_31','T_31','O_31']\n",
        "line_lst = ['T050304', 'T050307', 'T100304', 'T100306', 'T010306', 'T010305']"
      ],
      "metadata": {
        "id": "9LmUA9Eu1g0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['LINE'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHf13X89xAj",
        "outputId": "250ca372-dd5a-4a86-b78e-50194ee2e81c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['T050304', 'T050307', 'T100304', 'T100306', 'T010306', 'T010305'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    if i == 'LINE':\n",
        "      print(le.transform(line_lst))\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6fDHBnoXf3U",
        "outputId": "49af5dca-1c63-4831-d104-3564a7b94bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 1 0]\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_x[train_x['LINE'] == 2]\n",
        "train_b = train_x[train_x['LINE'] == 3]\n",
        "train_c = train_x[train_x['LINE'] == 4]\n",
        "train_d = train_x[train_x['LINE'] == 5]\n",
        "train_e = train_x[train_x['LINE'] == 1]\n",
        "train_f = train_x[train_x['LINE'] == 0]\n",
        "\n",
        "\n",
        "train_a_y = train_y[train_y['LINE'] == line_lst[0]]\n",
        "train_b_y = train_y[train_y['LINE'] == line_lst[1]]\n",
        "train_c_y = train_y[train_y['LINE'] == line_lst[2]]\n",
        "train_d_y = train_y[train_y['LINE'] == line_lst[3]]\n",
        "train_e_y = train_y[train_y['LINE'] == line_lst[4]]\n",
        "train_f_y = train_y[train_y['LINE'] == line_lst[5]]\n",
        "train_a_y = train_a_y['Y_Quality']\n",
        "train_b_y = train_b_y['Y_Quality']\n",
        "train_c_y = train_c_y['Y_Quality']\n",
        "train_d_y = train_d_y['Y_Quality']\n",
        "train_e_y = train_e_y['Y_Quality']\n",
        "train_f_y = train_f_y['Y_Quality']\n",
        "\n",
        "test_a = test_x[test_x['LINE'] == 2]\n",
        "test_b = test_x[test_x['LINE'] == 3]\n",
        "test_c = test_x[test_x['LINE'] == 4]\n",
        "test_d = test_x[test_x['LINE'] == 5]\n",
        "test_e = test_x[test_x['LINE'] == 1]\n",
        "test_f = test_x[test_x['LINE'] == 0]\n",
        "\n",
        "col = train_x.columns\n",
        "na_a_col = []\n",
        "na_b_col = []\n",
        "na_c_col = []\n",
        "na_d_col = []\n",
        "na_e_col = []\n",
        "na_f_col = []\n",
        "for i in col:\n",
        "  if train_a[i].count() == 0:\n",
        "    na_a_col.append(i)\n",
        "  if train_b[i].count() == 0:\n",
        "    na_b_col.append(i)\n",
        "  if train_c[i].count() == 0:\n",
        "    na_c_col.append(i)\n",
        "  if train_d[i].count() == 0:\n",
        "    na_d_col.append(i)\n",
        "  if train_e[i].count() == 0:\n",
        "    na_e_col.append(i)\n",
        "  if train_f[i].count() == 0:\n",
        "    na_f_col.append(i)\n",
        "\n",
        "train_a.drop(na_a_col, axis = 1, inplace = True)\n",
        "train_b.drop(na_b_col, axis = 1, inplace = True)\n",
        "train_c.drop(na_c_col, axis = 1, inplace = True)\n",
        "train_d.drop(na_d_col, axis = 1, inplace = True)\n",
        "train_e.drop(na_e_col, axis = 1, inplace = True)\n",
        "train_f.drop(na_f_col, axis = 1, inplace = True)\n",
        "\n",
        "test_a.drop(na_a_col, axis = 1, inplace = True)\n",
        "test_b.drop(na_b_col, axis = 1, inplace = True)\n",
        "test_c.drop(na_c_col, axis = 1, inplace = True)\n",
        "test_d.drop(na_d_col, axis = 1, inplace = True)\n",
        "test_e.drop(na_e_col, axis = 1, inplace = True)\n",
        "test_f.drop(na_f_col, axis = 1, inplace = True)\n",
        "print(f\"Shape of each Line : \\n{train_a.shape}  \\n{train_b.shape} \\n{train_c.shape}\\\n",
        " \\n{train_d.shape} \\n{train_e.shape} \\n{train_f.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sao3uefPIaG",
        "outputId": "1e828afb-cb11-4e9e-9abb-5a1a971fd005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of each Line : \n",
            "(78, 1970)  \n",
            "(42, 1977) \n",
            "(175, 673) \n",
            "(174, 673) \n",
            "(70, 888) \n",
            "(59, 888)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ceda027da552>:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_a.drop(na_a_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_b.drop(na_b_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_c.drop(na_c_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_d.drop(na_d_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_e.drop(na_e_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:55: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_f.drop(na_f_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_a.drop(na_a_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_b.drop(na_b_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_c.drop(na_c_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_d.drop(na_d_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:61: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_e.drop(na_e_col, axis = 1, inplace = True)\n",
            "<ipython-input-7-ceda027da552>:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_f.drop(na_f_col, axis = 1, inplace = True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_a.fillna(0)\n",
        "train_b = train_b.fillna(0)\n",
        "train_c = train_c.fillna(0)\n",
        "train_d = train_d.fillna(0)\n",
        "train_e = train_e.fillna(0)\n",
        "train_f = train_f.fillna(0)\n",
        "\n",
        "test_a = test_a.fillna(0)\n",
        "test_b = test_b.fillna(0)\n",
        "test_c = test_c.fillna(0)\n",
        "test_d = test_d.fillna(0)\n",
        "test_e = test_e.fillna(0)\n",
        "test_f = test_f.fillna(0)"
      ],
      "metadata": {
        "id": "BkCCcKGDUc28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 라인별로 특성값이 하나 밖에 없는 값 제거\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a,test_b,test_c,test_d,test_e,test_f]\n",
        "for idx, t_df in enumerate(train_total):\n",
        "  tmp = []\n",
        "  for i in t_df:\n",
        "    if i == 'LINE' or i == 'PRODUCT_CODE':\n",
        "      continue\n",
        "    if len(t_df[i].unique()) < 2:\n",
        "      tmp.append(i)\n",
        "  t_df.drop(tmp, axis = 1, inplace = True)\n",
        "  test_total[idx].drop(tmp, axis = 1, inplace = True)\n",
        "  print(f\"{line_lst[idx]} : {t_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JoSgZL4Myh",
        "outputId": "f06e29cf-2612-4670-c526-e632c995d351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T050304 : (78, 1722)\n",
            "T050307 : (42, 1726)\n",
            "T100304 : (175, 571)\n",
            "T100306 : (174, 543)\n",
            "T010306 : (70, 733)\n",
            "T010305 : (59, 732)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라쏘"
      ],
      "metadata": {
        "id": "uF4Icf-8U50A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "from lightgbm import log_evaluation"
      ],
      "metadata": {
        "id": "_R0kX7gkGsyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_a['Y_Quality'] = train_a_y.values\n",
        "train_b['Y_Quality'] = train_b_y.values\n",
        "train_c['Y_Quality'] = train_c_y.values\n",
        "train_d['Y_Quality'] = train_d_y.values\n",
        "train_e['Y_Quality'] = train_e_y.values\n",
        "train_f['Y_Quality'] = train_f_y.values"
      ],
      "metadata": {
        "id": "Bwm9Jyw2G6_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RF_a = TabularPredictor(label='Y_Quality').fit(train_data=train_a)\n",
        "RF_b = TabularPredictor(label='Y_Quality').fit(train_data=train_b)\n",
        "RF_c = TabularPredictor(label='Y_Quality').fit(train_data=train_c)\n",
        "RF_d = TabularPredictor(label='Y_Quality').fit(train_data=train_d)\n",
        "RF_e = TabularPredictor(label='Y_Quality').fit(train_data=train_e)\n",
        "RF_f = TabularPredictor(label='Y_Quality').fit(train_data=train_f)"
      ],
      "metadata": {
        "id": "i-v6c8q-Wl7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb13993-bf21-4bf4-feb4-5b92a43a41c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_092556/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_092556/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    78\n",
            "Train Data Columns: 1722\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.560568254, 0.513750794, 0.52795, 0.00837)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    12319.43 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.07 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 524 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1720 | ['X_128', 'X_129', 'X_132', 'X_133', 'X_134', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1196 | ['X_128', 'X_129', 'X_132', 'X_134', 'X_136', ...]\n",
            "\t\t('int', ['bool']) :  524 | ['X_133', 'X_143', 'X_151', 'X_152', 'X_155', ...]\n",
            "\t10.7s = Fit runtime\n",
            "\t1720 features in original data used to generate 1720 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 11.57s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 62, Val Rows: 16\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n",
            "\t-0.0141\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0138\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.43s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0117\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.2s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0118\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.04s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0111\t = Validation score   (-root_mean_squared_error)\n",
            "\t7.56s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.0113\t = Validation score   (-root_mean_squared_error)\n",
            "\t39.98s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0104\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.23s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-1.8802\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.67s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0094\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.74s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.0111\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.16s\t = Training   runtime\n",
            "\t0.22s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0089\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.91s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0089\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.36s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 86.79s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_092556/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_092724/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_092724/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    42\n",
            "Train Data Columns: 1726\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.57884127, 0.5158841270000001, 0.53574, 0.01284)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11808.65 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 140 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1724 | ['X_130', 'X_131', 'X_132', 'X_133', 'X_134', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1584 | ['X_130', 'X_131', 'X_136', 'X_137', 'X_138', ...]\n",
            "\t\t('int', ['bool']) :  140 | ['X_132', 'X_133', 'X_134', 'X_151', 'X_155', ...]\n",
            "\t2.2s = Fit runtime\n",
            "\t1724 features in original data used to generate 1724 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.45s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 33, Val Rows: 9\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.0172\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.3s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0145\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.25s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0172\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.02s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0172\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.96s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0143\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.04s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.0145\t = Validation score   (-root_mean_squared_error)\n",
            "\t54.88s\t = Training   runtime\n",
            "\t0.25s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0135\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.92s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.0111\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.07s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0157\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.33s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.011\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.98s\t = Training   runtime\n",
            "\t0.34s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0151\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.99s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0109\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.68s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 78.54s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_092724/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_092843/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_092843/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    175\n",
            "Train Data Columns: 571\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.545677778, 0.507571429, 0.52992, 0.0047)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11966.89 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.8 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 58 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 569 | ['X_1', 'X_2', 'X_5', 'X_11', 'X_12', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 512 | ['X_1', 'X_2', 'X_11', 'X_12', 'X_13', ...]\n",
            "\t\t('int', ['bool']) :  58 | ['PRODUCT_CODE', 'X_5', 'X_24', 'X_38', 'X_39', ...]\n",
            "\t1.0s = Fit runtime\n",
            "\t570 features in original data used to generate 570 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.73 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.09s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 140, Val Rows: 35\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.0051\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0046\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.13s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0045\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0042\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.17s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0047\t = Validation score   (-root_mean_squared_error)\n",
            "\t5.61s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.0043\t = Validation score   (-root_mean_squared_error)\n",
            "\t32.45s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0045\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.94s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 4: early stopping\n",
            "\t-0.0041\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.91s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.2s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.0039\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0045\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.74s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0039\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.36s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 54.46s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_092843/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_092937/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_092937/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    174\n",
            "Train Data Columns: 543\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.551279365, 0.50251746, 0.53073, 0.00475)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11963.24 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.76 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 34 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 541 | ['X_1', 'X_2', 'X_7', 'X_8', 'X_11', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 508 | ['X_1', 'X_2', 'X_7', 'X_11', 'X_12', ...]\n",
            "\t\t('int', ['bool']) :  34 | ['PRODUCT_CODE', 'X_8', 'X_15', 'X_24', 'X_39', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t542 features in original data used to generate 542 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.63s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 139, Val Rows: 35\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.0055\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0055\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0056\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.54s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0055\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.62s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.99s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.0053\t = Validation score   (-root_mean_squared_error)\n",
            "\t36.14s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.1s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 4: early stopping\n",
            "\t-0.0048\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0052\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.005\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.64s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0054\t = Validation score   (-root_mean_squared_error)\n",
            "\t3.01s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0047\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.59s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 53.82s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_092937/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_093031/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_093031/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    70\n",
            "Train Data Columns: 733\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.56031746, 0.513134921, 0.53431, 0.00907)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11960.46 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 731 | ['X_246', 'X_247', 'X_248', 'X_250', 'X_256', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 688 | ['X_246', 'X_247', 'X_248', 'X_256', 'X_257', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_250', 'X_268', 'X_270', 'X_271', 'X_272', ...]\n",
            "\t1.2s = Fit runtime\n",
            "\t731 features in original data used to generate 731 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.4s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 56, Val Rows: 14\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.0103\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.14s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0096\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.18s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0073\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0078\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0076\t = Validation score   (-root_mean_squared_error)\n",
            "\t4.32s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.008\t = Validation score   (-root_mean_squared_error)\n",
            "\t19.66s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0069\t = Validation score   (-root_mean_squared_error)\n",
            "\t2.59s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t-0.0075\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.84s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0046\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.0s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.0073\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.77s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0062\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.28s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0046\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.38s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 36.86s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_093031/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_093108/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_093108/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    59\n",
            "Train Data Columns: 732\n",
            "Label Column: Y_Quality\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (0.558769841, 0.500855556, 0.53067, 0.00878)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11961.75 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 730 | ['X_246', 'X_247', 'X_251', 'X_253', 'X_256', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 687 | ['X_246', 'X_247', 'X_251', 'X_256', 'X_257', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_253', 'X_269', 'X_271', 'X_273', 'X_275', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t730 features in original data used to generate 730 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.82s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 47, Val Rows: 12\n",
            "Fitting 11 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t-0.005\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t-0.0079\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t-0.0035\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.38s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t-0.0034\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.38s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE ...\n",
            "\t-0.0034\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.99s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t-0.0033\t = Validation score   (-root_mean_squared_error)\n",
            "\t16.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE ...\n",
            "\t-0.0029\t = Validation score   (-root_mean_squared_error)\n",
            "\t1.23s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 2: early stopping\n",
            "\t-0.0045\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t-0.0036\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.73s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t-0.0037\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.73s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t-0.0032\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.76s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t-0.0028\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.35s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 25.07s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_093108/\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_a = RF_a.predict(test_a).values\n",
        "pred_b = RF_b.predict(test_b).values\n",
        "pred_c = RF_c.predict(test_c).values\n",
        "pred_d = RF_d.predict(test_d).values\n",
        "pred_e = RF_e.predict(test_e).values\n",
        "pred_f = RF_f.predict(test_f).values"
      ],
      "metadata": {
        "id": "DMiDhHVIXomH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###pre"
      ],
      "metadata": {
        "id": "NNhoBUGo-ogI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df.drop(columns = ['TIMESTAMP','PRODUCT_ID','Y_Class'])\n",
        "train_y = train_df[['Y_Class','LINE']]\n",
        "test_x = test_df.drop(columns = ['TIMESTAMP','PRODUCT_ID'])"
      ],
      "metadata": {
        "id": "GCFxeQ5S-ogR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    if i == 'LINE':\n",
        "      print(le.transform(line_lst))\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3fc2f5-2566-4555-a7fc-414cda814a28",
        "id": "NmHNEZtf-ogR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 1 0]\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2nTOCgzK1G0",
        "outputId": "ce40682a-3a8f-4611-e795-dd895de850e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_x[train_x['LINE'] == 2]\n",
        "train_b = train_x[train_x['LINE'] == 3]\n",
        "train_c = train_x[train_x['LINE'] == 4]\n",
        "train_d = train_x[train_x['LINE'] == 5]\n",
        "train_e = train_x[train_x['LINE'] == 1]\n",
        "train_f = train_x[train_x['LINE'] == 0]\n",
        "\n",
        "\n",
        "train_a_y = train_y[train_y['LINE'] == line_lst[0]]\n",
        "train_b_y = train_y[train_y['LINE'] == line_lst[1]]\n",
        "train_c_y = train_y[train_y['LINE'] == line_lst[2]]\n",
        "train_d_y = train_y[train_y['LINE'] == line_lst[3]]\n",
        "train_e_y = train_y[train_y['LINE'] == line_lst[4]]\n",
        "train_f_y = train_y[train_y['LINE'] == line_lst[5]]\n",
        "train_a_y = train_a_y['Y_Class']\n",
        "train_b_y = train_b_y['Y_Class']\n",
        "train_c_y = train_c_y['Y_Class']\n",
        "train_d_y = train_d_y['Y_Class']\n",
        "train_e_y = train_e_y['Y_Class']\n",
        "train_f_y = train_f_y['Y_Class']\n",
        "\n",
        "test_a = test_x[test_x['LINE'] == 2]\n",
        "test_b = test_x[test_x['LINE'] == 3]\n",
        "test_c = test_x[test_x['LINE'] == 4]\n",
        "test_d = test_x[test_x['LINE'] == 5]\n",
        "test_e = test_x[test_x['LINE'] == 1]\n",
        "test_f = test_x[test_x['LINE'] == 0]\n",
        "\n",
        "col = train_x.columns\n",
        "na_a_col = []\n",
        "na_b_col = []\n",
        "na_c_col = []\n",
        "na_d_col = []\n",
        "na_e_col = []\n",
        "na_f_col = []\n",
        "for i in col:\n",
        "  if train_a[i].count() == 0:\n",
        "    na_a_col.append(i)\n",
        "  if train_b[i].count() == 0:\n",
        "    na_b_col.append(i)\n",
        "  if train_c[i].count() == 0:\n",
        "    na_c_col.append(i)\n",
        "  if train_d[i].count() == 0:\n",
        "    na_d_col.append(i)\n",
        "  if train_e[i].count() == 0:\n",
        "    na_e_col.append(i)\n",
        "  if train_f[i].count() == 0:\n",
        "    na_f_col.append(i)\n",
        "\n",
        "train_a = train_a.drop(na_a_col, axis = 1)\n",
        "train_b = train_b.drop(na_b_col, axis = 1)\n",
        "train_c = train_c.drop(na_c_col, axis = 1)\n",
        "train_d = train_d.drop(na_d_col, axis = 1)\n",
        "train_e = train_e.drop(na_e_col, axis = 1)\n",
        "train_f = train_f.drop(na_f_col, axis = 1)\n",
        "\n",
        "test_a = test_a.drop(na_a_col, axis = 1)\n",
        "test_b = test_b.drop(na_b_col, axis = 1)\n",
        "test_c = test_c.drop(na_c_col, axis = 1)\n",
        "test_d = test_d.drop(na_d_col, axis = 1)\n",
        "test_e = test_e.drop(na_e_col, axis = 1)\n",
        "test_f = test_f.drop(na_f_col, axis = 1)"
      ],
      "metadata": {
        "id": "mRea524D-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_a.fillna(0)\n",
        "train_b = train_b.fillna(0)\n",
        "train_c = train_c.fillna(0)\n",
        "train_d = train_d.fillna(0)\n",
        "train_e = train_e.fillna(0)\n",
        "train_f = train_f.fillna(0)\n",
        "\n",
        "test_a = test_a.fillna(0)\n",
        "test_b = test_b.fillna(0)\n",
        "test_c = test_c.fillna(0)\n",
        "test_d = test_d.fillna(0)\n",
        "test_e = test_e.fillna(0)\n",
        "test_f = test_f.fillna(0)"
      ],
      "metadata": {
        "id": "P_iL57NA-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 라인별로 특성값이 하나 밖에 없는 값 제거\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "for idx, t_df in enumerate(train_total):\n",
        "  tmp = []\n",
        "  for i in t_df:\n",
        "    if i == 'LINE' or i == 'PRODUCT_CODE':\n",
        "      continue\n",
        "    if len(t_df[i].unique()) < 2:\n",
        "      tmp.append(i)\n",
        "  t_df.drop(tmp, axis = 1, inplace = True)\n",
        "  test_total[idx].drop(tmp, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "g7UG3fUf-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_a['Y_Quality'] = pred_a\n",
        "test_b['Y_Quality'] = pred_b\n",
        "test_c['Y_Quality'] = pred_c\n",
        "test_d['Y_Quality'] = pred_d\n",
        "test_e['Y_Quality'] = pred_e\n",
        "test_f['Y_Quality'] = pred_f"
      ],
      "metadata": {
        "id": "-v8esOf6-_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cola = ['Y_Quality'] + test_a.columns.to_list()[:-1]\n",
        "new_colb = ['Y_Quality'] + test_b.columns.to_list()[:-1]\n",
        "new_colc = ['Y_Quality'] + test_c.columns.to_list()[:-1]\n",
        "new_cold = ['Y_Quality'] + test_d.columns.to_list()[:-1]\n",
        "new_cole = ['Y_Quality'] + test_e.columns.to_list()[:-1]\n",
        "new_colf = ['Y_Quality'] + test_f.columns.to_list()[:-1]\n",
        "test_a = test_a[new_cola]\n",
        "test_b = test_b[new_colb]\n",
        "test_c = test_c[new_colc]\n",
        "test_d = test_d[new_cold]\n",
        "test_e = test_e[new_cole]\n",
        "test_f = test_f[new_colf]"
      ],
      "metadata": {
        "id": "-fg9K5yHF8L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "# test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "# for k in range(len(train_total)):\n",
        "#   print(test_total[k].shape)\n",
        "#   for i in train_total[k].columns:\n",
        "#     mm = MinMaxScaler()\n",
        "#     fitted = mm.fit(train_total[k][i].values.reshape(-1,1))\n",
        "#     output = mm.transform(train_total[k][i].values.reshape(-1,1))\n",
        "#     out_test = mm.transform(test_total[k][i].values.reshape(-1,1))\n",
        "#     test_total[k][i] = out_test.reshape(-1)\n",
        "#     train_total[k][i] = output.reshape(-1)\n",
        "#   print(test_total[k].shape)"
      ],
      "metadata": {
        "id": "h_TPSlGtWKcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    train"
      ],
      "metadata": {
        "id": "kQX5_9jQsDVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from autogluon.tabular import TabularPredictor\n",
        "from lightgbm import log_evaluation\n",
        "\n",
        "RF_class = []\n",
        "y_preds = []\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "train_y_total = [train_a_y, train_b_y, train_c_y, train_d_y, train_e_y, train_f_y]\n",
        "\n",
        "for i in range(6):\n",
        "  train_total[i]['Y_Class'] = train_y_total[i].values\n",
        "  # autogluon \n",
        "  predictor = TabularPredictor(label='Y_Class').fit(train_data=train_total[i])\n",
        "  predictions = predictor.predict(test_total[i])\n",
        "  y_preds.append(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrxrvOAAsLVr",
        "outputId": "849940cd-ac6e-4b04-b340-661eb074aa6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_093908/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_093908/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    78\n",
            "Train Data Columns: 1723\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [1, 2, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11330.82 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 524 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1721 | ['Y_Quality', 'X_128', 'X_129', 'X_132', 'X_133', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1197 | ['Y_Quality', 'X_128', 'X_129', 'X_132', 'X_134', ...]\n",
            "\t\t('int', ['bool']) :  524 | ['X_133', 'X_143', 'X_151', 'X_152', 'X_155', ...]\n",
            "\t4.4s = Fit runtime\n",
            "\t1721 features in original data used to generate 1721 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 5.19s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 62, Val Rows: 16\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5625\t = Validation score   (accuracy)\n",
            "\t0.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.5625\t = Validation score   (accuracy)\n",
            "\t0.41s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.625\t = Validation score   (accuracy)\n",
            "\t3.49s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.75\t = Validation score   (accuracy)\n",
            "\t2.1s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t2.06s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t1.53s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t1.72s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tMany features detected (1720), dynamically setting 'colsample_bylevel' to 0.5813953488372093 to speed up training (Default = 1).\n",
            "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t140.72s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t1.42s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t2.04s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.97s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.625\t = Validation score   (accuracy)\n",
            "\t3.52s\t = Training   runtime\n",
            "\t0.5s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t6.3s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.26s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 176.48s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_093908/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_094205/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_094205/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    42\n",
            "Train Data Columns: 1727\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [2, 1, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Warning: Updated label_count_threshold from 10 to 9 to avoid cutting too many classes.\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11594.38 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 140 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1725 | ['Y_Quality', 'X_130', 'X_131', 'X_132', 'X_133', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1585 | ['Y_Quality', 'X_130', 'X_131', 'X_136', 'X_137', ...]\n",
            "\t\t('int', ['bool']) :  140 | ['X_132', 'X_133', 'X_134', 'X_151', 'X_155', ...]\n",
            "\t2.8s = Fit runtime\n",
            "\t1725 features in original data used to generate 1725 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 3.21s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 33, Val Rows: 9\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.27s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.25s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 0: early stopping\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.92s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.5556\t = Validation score   (accuracy)\n",
            "\t1.01s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.5556\t = Validation score   (accuracy)\n",
            "\t0.97s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.12s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.11s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tMany features detected (1722), dynamically setting 'colsample_bylevel' to 0.5807200929152149 to speed up training (Default = 1).\n",
            "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t47.76s\t = Training   runtime\n",
            "\t0.15s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8889\t = Validation score   (accuracy)\n",
            "\t2.14s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.4s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.34s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.97s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.27s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 71.12s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_094205/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_094317/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_094317/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    175\n",
            "Train Data Columns: 572\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [0, 1, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11594.8 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.8 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 58 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 570 | ['Y_Quality', 'X_1', 'X_2', 'X_5', 'X_11', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 513 | ['Y_Quality', 'X_1', 'X_2', 'X_11', 'X_12', ...]\n",
            "\t\t('int', ['bool']) :  58 | ['PRODUCT_CODE', 'X_5', 'X_24', 'X_38', 'X_39', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t571 features in original data used to generate 571 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.73 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.67s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 140, Val Rows: 35\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.06s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 3: early stopping\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t2.62s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t1.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t2.07s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.37s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.39s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t56.83s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.77s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t1.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.63s\t = Training   runtime\n",
            "\t0.27s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t7.88s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 81.61s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_094317/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_094439/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_094439/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    174\n",
            "Train Data Columns: 544\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [1, 0, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11594.56 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.76 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 34 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 542 | ['Y_Quality', 'X_1', 'X_2', 'X_7', 'X_8', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 509 | ['Y_Quality', 'X_1', 'X_2', 'X_7', 'X_11', ...]\n",
            "\t\t('int', ['bool']) :  34 | ['PRODUCT_CODE', 'X_8', 'X_15', 'X_24', 'X_39', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t543 features in original data used to generate 543 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.64s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 139, Val Rows: 35\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.7714\t = Validation score   (accuracy)\n",
            "\t0.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t2.3s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.16s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t1.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.78s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t30.68s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.16s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.54s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.31s\t = Training   runtime\n",
            "\t0.11s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t4.6s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 48.74s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_094439/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_094528/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_094528/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    70\n",
            "Train Data Columns: 734\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [2, 1, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Warning: Updated label_count_threshold from 10 to 6 to avoid cutting too many classes.\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11593.98 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 732 | ['Y_Quality', 'X_246', 'X_247', 'X_248', 'X_250', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 689 | ['Y_Quality', 'X_246', 'X_247', 'X_248', 'X_256', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_250', 'X_268', 'X_270', 'X_271', 'X_272', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t732 features in original data used to generate 732 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.83s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 56, Val Rows: 14\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5\t = Validation score   (accuracy)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6429\t = Validation score   (accuracy)\n",
            "\t0.12s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 3: early stopping\n",
            "\t0.7857\t = Validation score   (accuracy)\n",
            "\t1.19s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t0.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.7857\t = Validation score   (accuracy)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t26.17s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.7143\t = Validation score   (accuracy)\n",
            "\t1.25s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.23s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.21s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.7143\t = Validation score   (accuracy)\n",
            "\t1.44s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t4.05s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.26s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 41.68s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_094528/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230220_094610/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230220_094610/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    59\n",
            "Train Data Columns: 733\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [0, 1, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11593.93 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 731 | ['Y_Quality', 'X_246', 'X_247', 'X_251', 'X_253', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 688 | ['Y_Quality', 'X_246', 'X_247', 'X_251', 'X_256', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_253', 'X_269', 'X_271', 'X_273', 'X_275', ...]\n",
            "\t0.7s = Fit runtime\n",
            "\t731 features in original data used to generate 731 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.88s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 47, Val Rows: 12\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5\t = Validation score   (accuracy)\n",
            "\t0.09s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.08s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t1.23s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9167\t = Validation score   (accuracy)\n",
            "\t0.5s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t0.79s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.82s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t33.57s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t1.21s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t1.2s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.0s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t1.17s\t = Training   runtime\n",
            "\t0.13s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.02s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.26s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 46.34s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230220_094610/\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHXZJ4i1EGYY",
        "outputId": "d14b17a8-9442-4424-db09-054b3d4f6bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  y_preds[i] = y_preds[i].values"
      ],
      "metadata": {
        "id": "shqayDnU4bW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = []\n",
        "cnt = [0 for i in range(6)]\n",
        "test_prd = test_x['LINE']\n",
        "for idx,i in enumerate(test_prd):\n",
        "  if i == 2:\n",
        "    test_pred.append(y_preds[0][cnt[0]])\n",
        "    cnt[0] += 1\n",
        "  elif i == 3:\n",
        "    test_pred.append(y_preds[1][cnt[1]])\n",
        "    cnt[1] += 1\n",
        "  elif i == 4:\n",
        "    test_pred.append(y_preds[2][cnt[2]])\n",
        "    cnt[2] += 1\n",
        "  elif i == 5:\n",
        "    test_pred.append(y_preds[3][cnt[3]])\n",
        "    cnt[3] += 1\n",
        "  elif i == 1:\n",
        "    test_pred.append(y_preds[4][cnt[4]])\n",
        "    cnt[4] += 1\n",
        "  else:\n",
        "    test_pred.append(y_preds[5][cnt[5]])\n",
        "    cnt[5] += 1"
      ],
      "metadata": {
        "id": "O1YdJuDK_eh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_pred)):\n",
        "  print(test_pred[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4tIJTjbFixX",
        "outputId": "8ab4c6e8-a6bf-4147-98ee-1036477494b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "2\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "1\n",
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save"
      ],
      "metadata": {
        "id": "NuD8ODK9Yl3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['Y_Class'] = test_pred\n",
        "path = 'baselinne.csv'\n",
        "submit.to_csv(path, index = False)"
      ],
      "metadata": {
        "id": "xa3zmTaPAIRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "1d6fd6dc-6bde-4f33-ba2c-4f6d42029ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32mnewEDA.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'baselinne.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m             \u001b[0msparsify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparsify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m             \u001b[0mindex_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m             \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimplementing\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0mobject\u001b[0m \u001b[0mimplementing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mreturned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibwriters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mCompressionOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mFilePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FilePath' from 'pandas._typing' (/usr/local/lib/python3.8/dist-packages/pandas/_typing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC3cglpN5Qvz",
        "outputId": "272346ae-ed27-4d91-9eb7-2e94936a21f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
          ]
        }
      ]
    }
  ]
}