{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fiwK03v1P3tJ"
      ],
      "authorship_tag": "ABX9TyOSQH1j9xTdOPn8BrYUPyuc"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###pre"
      ],
      "metadata": {
        "id": "fiwK03v1P3tJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQWtFd32OLUA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier,RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso,Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('./train.csv')\n",
        "test_df = pd.read_csv('./test.csv')"
      ],
      "metadata": {
        "id": "sPTuTNhnOseO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df.drop(columns = ['TIMESTAMP','PRODUCT_ID', 'Y_Quality','Y_Class'])\n",
        "train_y = train_df[['Y_Quality','LINE']]\n",
        "test_x = test_df.drop(columns = ['TIMESTAMP','PRODUCT_ID'])"
      ],
      "metadata": {
        "id": "GA8er46iO1oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "product_lst = ['A_31','T_31','O_31']\n",
        "line_lst = ['T050304', 'T050307', 'T100304', 'T100306', 'T010306', 'T010305']"
      ],
      "metadata": {
        "id": "9LmUA9Eu1g0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['LINE'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AHf13X89xAj",
        "outputId": "14afd8a6-e94a-4b08-c6de-ad66b6f0011a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['T050304', 'T050307', 'T100304', 'T100306', 'T010306', 'T010305'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    if i == 'LINE':\n",
        "      print(le.transform(line_lst))\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6fDHBnoXf3U",
        "outputId": "5086afbe-c383-4c9a-cf8f-5414a4313a36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 1 0]\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_x[train_x['LINE'] == 2]\n",
        "train_b = train_x[train_x['LINE'] == 3]\n",
        "train_c = train_x[train_x['LINE'] == 4]\n",
        "train_d = train_x[train_x['LINE'] == 5]\n",
        "train_e = train_x[train_x['LINE'] == 1]\n",
        "train_f = train_x[train_x['LINE'] == 0]\n",
        "\n",
        "\n",
        "train_a_y = train_y[train_y['LINE'] == line_lst[0]]\n",
        "train_b_y = train_y[train_y['LINE'] == line_lst[1]]\n",
        "train_c_y = train_y[train_y['LINE'] == line_lst[2]]\n",
        "train_d_y = train_y[train_y['LINE'] == line_lst[3]]\n",
        "train_e_y = train_y[train_y['LINE'] == line_lst[4]]\n",
        "train_f_y = train_y[train_y['LINE'] == line_lst[5]]\n",
        "train_a_y = train_a_y['Y_Quality']\n",
        "train_b_y = train_b_y['Y_Quality']\n",
        "train_c_y = train_c_y['Y_Quality']\n",
        "train_d_y = train_d_y['Y_Quality']\n",
        "train_e_y = train_e_y['Y_Quality']\n",
        "train_f_y = train_f_y['Y_Quality']\n",
        "\n",
        "test_a = test_x[test_x['LINE'] == 2]\n",
        "test_b = test_x[test_x['LINE'] == 3]\n",
        "test_c = test_x[test_x['LINE'] == 4]\n",
        "test_d = test_x[test_x['LINE'] == 5]\n",
        "test_e = test_x[test_x['LINE'] == 1]\n",
        "test_f = test_x[test_x['LINE'] == 0]\n",
        "\n",
        "col = train_x.columns\n",
        "na_a_col = []\n",
        "na_b_col = []\n",
        "na_c_col = []\n",
        "na_d_col = []\n",
        "na_e_col = []\n",
        "na_f_col = []\n",
        "for i in col:\n",
        "  if train_a[i].count() == 0:\n",
        "    na_a_col.append(i)\n",
        "  if train_b[i].count() == 0:\n",
        "    na_b_col.append(i)\n",
        "  if train_c[i].count() == 0:\n",
        "    na_c_col.append(i)\n",
        "  if train_d[i].count() == 0:\n",
        "    na_d_col.append(i)\n",
        "  if train_e[i].count() == 0:\n",
        "    na_e_col.append(i)\n",
        "  if train_f[i].count() == 0:\n",
        "    na_f_col.append(i)\n",
        "\n",
        "train_a = train_a.drop(na_a_col, axis = 1)\n",
        "train_b = train_b.drop(na_b_col, axis = 1)\n",
        "train_c = train_c.drop(na_c_col, axis = 1)\n",
        "train_d = train_d.drop(na_d_col, axis = 1)\n",
        "train_e = train_e.drop(na_e_col, axis = 1)\n",
        "train_f = train_f.drop(na_f_col, axis = 1)\n",
        "\n",
        "test_a = test_a.drop(na_a_col, axis = 1)\n",
        "test_b = test_b.drop(na_b_col, axis = 1)\n",
        "test_c = test_c.drop(na_c_col, axis = 1)\n",
        "test_d = test_d.drop(na_d_col, axis = 1)\n",
        "test_e = test_e.drop(na_e_col, axis = 1)\n",
        "test_f = test_f.drop(na_f_col, axis = 1)\n",
        "print(f\"Shape of each Line : \\n{train_a.shape}  \\n{train_b.shape} \\n{train_c.shape}\\\n",
        " \\n{train_d.shape} \\n{train_e.shape} \\n{train_f.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sao3uefPIaG",
        "outputId": "c2dbd792-53e9-44fd-b7f1-5082e0f08d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of each Line : \n",
            "(78, 1970)  \n",
            "(42, 1977) \n",
            "(175, 673) \n",
            "(174, 673) \n",
            "(70, 888) \n",
            "(59, 888)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_a.fillna(0)\n",
        "train_b = train_b.fillna(0)\n",
        "train_c = train_c.fillna(0)\n",
        "train_d = train_d.fillna(0)\n",
        "train_e = train_e.fillna(0)\n",
        "train_f = train_f.fillna(0)\n",
        "\n",
        "test_a = test_a.fillna(0)\n",
        "test_b = test_b.fillna(0)\n",
        "test_c = test_c.fillna(0)\n",
        "test_d = test_d.fillna(0)\n",
        "test_e = test_e.fillna(0)\n",
        "test_f = test_f.fillna(0)"
      ],
      "metadata": {
        "id": "BkCCcKGDUc28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 라인별로 특성값이 하나 밖에 없는 값 제거\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a,test_b,test_c,test_d,test_e,test_f]\n",
        "for idx, t_df in enumerate(train_total):\n",
        "  tmp = []\n",
        "  for i in t_df:\n",
        "    if i == 'LINE' or i == 'PRODUCT_CODE':\n",
        "      continue\n",
        "    if len(t_df[i].unique()) < 2:\n",
        "      tmp.append(i)\n",
        "  t_df.drop(tmp, axis = 1, inplace = True)\n",
        "  test_total[idx].drop(tmp, axis = 1, inplace = True)\n",
        "  print(f\" {t_df.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4JoSgZL4Myh",
        "outputId": "8d25e6cc-31af-43d4-ca8c-f3542d499674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " (78, 1722)\n",
            " (42, 1726)\n",
            " (175, 571)\n",
            " (174, 543)\n",
            " (70, 733)\n",
            " (59, 732)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 라쏘"
      ],
      "metadata": {
        "id": "uF4Icf-8U50A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF_a = RandomForestRegressor(max_depth = 10, min_samples_leaf = 2,\\\n",
        "          min_samples_split = 10,n_estimators = 10, n_jobs = -1).fit(train_a, train_a_y)\n",
        "RF_b = RandomForestRegressor(max_depth = 40, min_samples_leaf = 4,\\\n",
        "          min_samples_split = 10,n_estimators = 20, n_jobs = -1).fit(train_b, train_b_y)\n",
        "RF_c = RandomForestRegressor(max_depth = 20, min_samples_leaf = 10,\\\n",
        "          min_samples_split = 50,n_estimators = 20, n_jobs = -1).fit(train_c, train_c_y)\n",
        "RF_d = RandomForestRegressor(max_depth = 20, min_samples_leaf = 2,\\\n",
        "          min_samples_split = 20,n_estimators = 20, n_jobs = -1).fit(train_d, train_d_y)\n",
        "RF_e = RandomForestRegressor(max_depth = 30, min_samples_leaf = 4,\\\n",
        "          min_samples_split = 10,n_estimators = 10, n_jobs = -1).fit(train_e, train_e_y)\n",
        "RF_f = RandomForestRegressor(max_depth = 12, min_samples_leaf = 2,\\\n",
        "          min_samples_split = 20,n_estimators = 30, n_jobs = -1).fit(train_f, train_f_y)"
      ],
      "metadata": {
        "id": "D0g-4zJ550AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# params = { 'n_estimators' : [10,100,500,1000,2000],\n",
        "#            'max_depth' : [ 10, 12,14,16],\n",
        "#            'min_samples_leaf' : [8,10,12,14],\n",
        "#            'min_samples_split' : [8,12, 16, 20]\n",
        "#             }\n",
        "# grid_cv = GridSearchCV(RF_a, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_a, train_a_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# grid_cv = GridSearchCV(RF_b, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_b, train_b_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# grid_cv = GridSearchCV(RF_c, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_c, train_c_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# grid_cv = GridSearchCV(RF_d, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_d, train_d_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# grid_cv = GridSearchCV(RF_e, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_e, train_e_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "\n",
        "# grid_cv = GridSearchCV(RF_f, param_grid = params, cv = 3, n_jobs = -1)\n",
        "# grid_cv.fit(train_f, train_f_y)\n",
        "# print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "# print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
      ],
      "metadata": {
        "id": "PytLQXuBW7os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_a = RF_a.predict(test_a)\n",
        "pred_b = RF_b.predict(test_b)\n",
        "pred_c = RF_c.predict(test_c)\n",
        "pred_d = RF_d.predict(test_d)\n",
        "pred_e = RF_e.predict(test_e)\n",
        "pred_f = RF_f.predict(test_f)"
      ],
      "metadata": {
        "id": "DMiDhHVIXomH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###pre"
      ],
      "metadata": {
        "id": "NNhoBUGo-ogI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = train_df.drop(columns = ['TIMESTAMP','PRODUCT_ID','Y_Class'])\n",
        "train_y = train_df[['Y_Class','LINE']]\n",
        "test_x = test_df.drop(columns = ['TIMESTAMP','PRODUCT_ID'])"
      ],
      "metadata": {
        "id": "GCFxeQ5S-ogR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qual_col = ['LINE', 'PRODUCT_CODE']\n",
        "\n",
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    if i == 'LINE':\n",
        "      print(le.transform(line_lst))\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "684106d5-0684-4a52-e3a7-69fdaa04da0d",
        "id": "NmHNEZtf-ogR"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 3 4 5 1 0]\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_x[train_x['LINE'] == 2]\n",
        "train_b = train_x[train_x['LINE'] == 3]\n",
        "train_c = train_x[train_x['LINE'] == 4]\n",
        "train_d = train_x[train_x['LINE'] == 5]\n",
        "train_e = train_x[train_x['LINE'] == 1]\n",
        "train_f = train_x[train_x['LINE'] == 0]\n",
        "\n",
        "\n",
        "train_a_y = train_y[train_y['LINE'] == line_lst[0]]\n",
        "train_b_y = train_y[train_y['LINE'] == line_lst[1]]\n",
        "train_c_y = train_y[train_y['LINE'] == line_lst[2]]\n",
        "train_d_y = train_y[train_y['LINE'] == line_lst[3]]\n",
        "train_e_y = train_y[train_y['LINE'] == line_lst[4]]\n",
        "train_f_y = train_y[train_y['LINE'] == line_lst[5]]\n",
        "train_a_y = train_a_y['Y_Class']\n",
        "train_b_y = train_b_y['Y_Class']\n",
        "train_c_y = train_c_y['Y_Class']\n",
        "train_d_y = train_d_y['Y_Class']\n",
        "train_e_y = train_e_y['Y_Class']\n",
        "train_f_y = train_f_y['Y_Class']\n",
        "\n",
        "test_a = test_x[test_x['LINE'] == 2]\n",
        "test_b = test_x[test_x['LINE'] == 3]\n",
        "test_c = test_x[test_x['LINE'] == 4]\n",
        "test_d = test_x[test_x['LINE'] == 5]\n",
        "test_e = test_x[test_x['LINE'] == 1]\n",
        "test_f = test_x[test_x['LINE'] == 0]\n",
        "\n",
        "col = train_x.columns\n",
        "na_a_col = []\n",
        "na_b_col = []\n",
        "na_c_col = []\n",
        "na_d_col = []\n",
        "na_e_col = []\n",
        "na_f_col = []\n",
        "for i in col:\n",
        "  if train_a[i].count() == 0:\n",
        "    na_a_col.append(i)\n",
        "  if train_b[i].count() == 0:\n",
        "    na_b_col.append(i)\n",
        "  if train_c[i].count() == 0:\n",
        "    na_c_col.append(i)\n",
        "  if train_d[i].count() == 0:\n",
        "    na_d_col.append(i)\n",
        "  if train_e[i].count() == 0:\n",
        "    na_e_col.append(i)\n",
        "  if train_f[i].count() == 0:\n",
        "    na_f_col.append(i)\n",
        "\n",
        "train_a = train_a.drop(na_a_col, axis = 1)\n",
        "train_b = train_b.drop(na_b_col, axis = 1)\n",
        "train_c = train_c.drop(na_c_col, axis = 1)\n",
        "train_d = train_d.drop(na_d_col, axis = 1)\n",
        "train_e = train_e.drop(na_e_col, axis = 1)\n",
        "train_f = train_f.drop(na_f_col, axis = 1)\n",
        "\n",
        "test_a = test_a.drop(na_a_col, axis = 1)\n",
        "test_b = test_b.drop(na_b_col, axis = 1)\n",
        "test_c = test_c.drop(na_c_col, axis = 1)\n",
        "test_d = test_d.drop(na_d_col, axis = 1)\n",
        "test_e = test_e.drop(na_e_col, axis = 1)\n",
        "test_f = test_f.drop(na_f_col, axis = 1)"
      ],
      "metadata": {
        "id": "mRea524D-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_a = train_a.fillna(0)\n",
        "train_b = train_b.fillna(0)\n",
        "train_c = train_c.fillna(0)\n",
        "train_d = train_d.fillna(0)\n",
        "train_e = train_e.fillna(0)\n",
        "train_f = train_f.fillna(0)\n",
        "\n",
        "test_a = test_a.fillna(0)\n",
        "test_b = test_b.fillna(0)\n",
        "test_c = test_c.fillna(0)\n",
        "test_d = test_d.fillna(0)\n",
        "test_e = test_e.fillna(0)\n",
        "test_f = test_f.fillna(0)"
      ],
      "metadata": {
        "id": "P_iL57NA-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 라인별로 특성값이 하나 밖에 없는 값 제거\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "for idx, t_df in enumerate(train_total):\n",
        "  tmp = []\n",
        "  for i in t_df:\n",
        "    if i == 'LINE' or i == 'PRODUCT_CODE':\n",
        "      continue\n",
        "    if len(t_df[i].unique()) < 2:\n",
        "      tmp.append(i)\n",
        "  t_df.drop(tmp, axis = 1, inplace = True)\n",
        "  test_total[idx].drop(tmp, axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "g7UG3fUf-ogS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_a['Y_Quality'] = pred_a\n",
        "test_b['Y_Quality'] = pred_b\n",
        "test_c['Y_Quality'] = pred_c\n",
        "test_d['Y_Quality'] = pred_d\n",
        "test_e['Y_Quality'] = pred_e\n",
        "test_f['Y_Quality'] = pred_f"
      ],
      "metadata": {
        "id": "-v8esOf6-_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cola = ['Y_Quality'] + test_a.columns.to_list()[:-1]\n",
        "new_colb = ['Y_Quality'] + test_b.columns.to_list()[:-1]\n",
        "new_colc = ['Y_Quality'] + test_c.columns.to_list()[:-1]\n",
        "new_cold = ['Y_Quality'] + test_d.columns.to_list()[:-1]\n",
        "new_cole = ['Y_Quality'] + test_e.columns.to_list()[:-1]\n",
        "new_colf = ['Y_Quality'] + test_f.columns.to_list()[:-1]\n",
        "test_a = test_a[new_cola]\n",
        "test_b = test_b[new_colb]\n",
        "test_c = test_c[new_colc]\n",
        "test_d = test_d[new_cold]\n",
        "test_e = test_e[new_cole]\n",
        "test_f = test_f[new_colf]"
      ],
      "metadata": {
        "id": "-fg9K5yHF8L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "# test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "# for k in range(len(train_total)):\n",
        "#   print(test_total[k].shape)\n",
        "#   for i in train_total[k].columns:\n",
        "#     mm = MinMaxScaler()\n",
        "#     fitted = mm.fit(train_total[k][i].values.reshape(-1,1))\n",
        "#     output = mm.transform(train_total[k][i].values.reshape(-1,1))\n",
        "#     out_test = mm.transform(test_total[k][i].values.reshape(-1,1))\n",
        "#     test_total[k][i] = out_test.reshape(-1)\n",
        "#     train_total[k][i] = output.reshape(-1)\n",
        "#   print(test_total[k].shape)"
      ],
      "metadata": {
        "id": "h_TPSlGtWKcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#    train"
      ],
      "metadata": {
        "id": "kQX5_9jQsDVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from autogluon.tabular import TabularPredictor\n",
        "# from lightgbm import log_evaluation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "RF_class = []\n",
        "y_preds = []\n",
        "train_total = [train_a, train_b, train_c, train_d, train_e, train_f]\n",
        "test_total = [test_a, test_b,test_c,test_d,test_e,test_f]\n",
        "train_y_total = [train_a_y, train_b_y, train_c_y, train_d_y, train_e_y, train_f_y]\n",
        "\n",
        "for i in range(6):\n",
        "  train_total[i]['Y_Class'] = train_y_total[i].values\n",
        "  # autogluon \n",
        "  predictor = TabularPredictor(label='Y_Class').fit(train_data=train_total[i])\n",
        "  predictions = predictor.predict(test_total[i])\n",
        "  # RF = LogisticRegression(n_jobs = -1)\n",
        "\n",
        "  # params = { 'penalty' : ['l1', 'l2', 'elasticnet'],\n",
        "  #             'dual' : [True,False],\n",
        "  #             'tol' : [1e-2, 1e-3,1e-4,1e-5],\n",
        "  #           'solver' : ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
        "  #           'max_iter' : [100,200,300,400,500],\n",
        "  #           'C': [0.0001,0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "  #             }\n",
        "  # grid_cv = GridSearchCV(RF, param_grid = params, cv = 3, n_jobs = -1)\n",
        "  # grid_cv.fit(train_total[i], train_y_total[i])\n",
        "  # print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
        "  # print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrxrvOAAsLVr",
        "outputId": "8bf7172c-40e5-415a-8220-a8c69f2188e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_181825/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_181825/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    78\n",
            "Train Data Columns: 1723\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [1, 2, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11787.23 MB\n",
            "\tTrain Data (Original)  Memory Usage: 1.08 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 524 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1721 | ['Y_Quality', 'X_128', 'X_129', 'X_132', 'X_133', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1197 | ['Y_Quality', 'X_128', 'X_129', 'X_132', 'X_134', ...]\n",
            "\t\t('int', ['bool']) :  524 | ['X_133', 'X_143', 'X_151', 'X_152', 'X_155', ...]\n",
            "\t3.2s = Fit runtime\n",
            "\t1721 features in original data used to generate 1721 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.79 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 3.47s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 62, Val Rows: 16\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5625\t = Validation score   (accuracy)\n",
            "\t0.24s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.5625\t = Validation score   (accuracy)\n",
            "\t0.28s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.625\t = Validation score   (accuracy)\n",
            "\t2.23s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.75\t = Validation score   (accuracy)\n",
            "\t1.96s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t2.0s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t2.11s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t2.13s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tMany features detected (1720), dynamically setting 'colsample_bylevel' to 0.5813953488372093 to speed up training (Default = 1).\n",
            "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t147.63s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t1.25s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.6875\t = Validation score   (accuracy)\n",
            "\t1.43s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.79s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.625\t = Validation score   (accuracy)\n",
            "\t3.24s\t = Training   runtime\n",
            "\t0.46s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9375\t = Validation score   (accuracy)\n",
            "\t6.65s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.25s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 179.67s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_181825/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_182126/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_182126/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    42\n",
            "Train Data Columns: 1727\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [2, 1, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Warning: Updated label_count_threshold from 10 to 9 to avoid cutting too many classes.\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11802.01 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.58 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 140 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 1725 | ['Y_Quality', 'X_130', 'X_131', 'X_132', 'X_133', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 1585 | ['Y_Quality', 'X_130', 'X_131', 'X_136', 'X_137', ...]\n",
            "\t\t('int', ['bool']) :  140 | ['X_132', 'X_133', 'X_134', 'X_151', 'X_155', ...]\n",
            "\t2.1s = Fit runtime\n",
            "\t1725 features in original data used to generate 1725 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.54 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 2.34s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 33, Val Rows: 9\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.31s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.3s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 0: early stopping\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.83s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.5556\t = Validation score   (accuracy)\n",
            "\t0.63s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.5556\t = Validation score   (accuracy)\n",
            "\t0.93s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.3s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.0s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\tMany features detected (1722), dynamically setting 'colsample_bylevel' to 0.5807200929152149 to speed up training (Default = 1).\n",
            "\tTo disable this functionality, explicitly specify 'colsample_bylevel' in the model hyperparameters.\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t46.82s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8889\t = Validation score   (accuracy)\n",
            "\t2.13s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.99s\t = Training   runtime\n",
            "\t0.08s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.98s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.54s\t = Training   runtime\n",
            "\t0.18s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.49s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.24s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 69.52s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_182126/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_182235/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_182235/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    175\n",
            "Train Data Columns: 572\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [0, 1, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11847.05 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.8 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 58 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 570 | ['Y_Quality', 'X_1', 'X_2', 'X_5', 'X_11', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 513 | ['Y_Quality', 'X_1', 'X_2', 'X_11', 'X_12', ...]\n",
            "\t\t('int', ['bool']) :  58 | ['PRODUCT_CODE', 'X_5', 'X_24', 'X_38', 'X_39', ...]\n",
            "\t0.5s = Fit runtime\n",
            "\t571 features in original data used to generate 571 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.73 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.65s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 140, Val Rows: 35\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.06s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t0.1s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 3: early stopping\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t1.77s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8857\t = Validation score   (accuracy)\n",
            "\t1.07s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t1.2s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.81s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t60.11s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.7s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.72s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t1.0s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.87s\t = Training   runtime\n",
            "\t0.14s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t4.9s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t0.9714\t = Validation score   (accuracy)\n",
            "\t0.45s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 76.48s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_182235/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_182352/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_182352/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    174\n",
            "Train Data Columns: 544\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [1, 0, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11846.25 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.76 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 34 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 1): ['LINE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 542 | ['Y_Quality', 'X_1', 'X_2', 'X_7', 'X_8', ...]\n",
            "\t\t('int', [])   :   1 | ['PRODUCT_CODE']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 509 | ['Y_Quality', 'X_1', 'X_2', 'X_7', 'X_11', ...]\n",
            "\t\t('int', ['bool']) :  34 | ['PRODUCT_CODE', 'X_8', 'X_15', 'X_24', 'X_39', ...]\n",
            "\t0.9s = Fit runtime\n",
            "\t543 features in original data used to generate 543 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.71 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.01s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 139, Val Rows: 35\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.7714\t = Validation score   (accuracy)\n",
            "\t0.11s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t2.95s\t = Training   runtime\n",
            "\t0.03s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t1.78s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t1.26s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.04s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t28.3s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.04s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.04s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t1.45s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.8286\t = Validation score   (accuracy)\n",
            "\t1.23s\t = Training   runtime\n",
            "\t0.1s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9429\t = Validation score   (accuracy)\n",
            "\t7.38s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.49s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 52.14s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_182352/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_182444/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_182444/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    70\n",
            "Train Data Columns: 734\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [2, 1, 0]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Warning: Updated label_count_threshold from 10 to 6 to avoid cutting too many classes.\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11845.9 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.41 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 732 | ['Y_Quality', 'X_246', 'X_247', 'X_248', 'X_250', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 689 | ['Y_Quality', 'X_246', 'X_247', 'X_248', 'X_256', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_250', 'X_268', 'X_270', 'X_271', 'X_272', ...]\n",
            "\t1.2s = Fit runtime\n",
            "\t732 features in original data used to generate 732 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.39 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.33s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 56, Val Rows: 14\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5\t = Validation score   (accuracy)\n",
            "\t0.15s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6429\t = Validation score   (accuracy)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "No improvement since epoch 3: early stopping\n",
            "\t0.7857\t = Validation score   (accuracy)\n",
            "\t1.78s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.68s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t0.56s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.7857\t = Validation score   (accuracy)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.81s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t25.28s\t = Training   runtime\n",
            "\t0.04s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.7143\t = Validation score   (accuracy)\n",
            "\t0.75s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.8571\t = Validation score   (accuracy)\n",
            "\t0.81s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.66s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.7143\t = Validation score   (accuracy)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t0.9286\t = Validation score   (accuracy)\n",
            "\t3.4s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.44s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 39.31s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_182444/\")\n",
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20230221_182524/\"\n",
            "Beginning AutoGluon training ...\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20230221_182524/\"\n",
            "AutoGluon Version:  0.7.0\n",
            "Python Version:     3.8.10\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Sat Dec 10 16:00:40 UTC 2022\n",
            "Train Data Rows:    59\n",
            "Train Data Columns: 733\n",
            "Label Column: Y_Class\n",
            "Preprocessing data ...\n",
            "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
            "\t3 unique label values:  [0, 1, 2]\n",
            "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
            "Train Data Class Count: 3\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    11845.35 MB\n",
            "\tTrain Data (Original)  Memory Usage: 0.35 MB (0.0% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 43 features to boolean dtype as they only contain 2 unique values.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tUseless Original Features (Count: 2): ['LINE', 'PRODUCT_CODE']\n",
            "\t\tThese features carry no predictive signal and should be manually investigated.\n",
            "\t\tThis is typically a feature which has the same value for all rows.\n",
            "\t\tThese features do not need to be present at inference time.\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', []) : 731 | ['Y_Quality', 'X_246', 'X_247', 'X_251', 'X_253', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('float', [])     : 688 | ['Y_Quality', 'X_246', 'X_247', 'X_251', 'X_256', ...]\n",
            "\t\t('int', ['bool']) :  43 | ['X_253', 'X_269', 'X_271', 'X_273', 'X_275', ...]\n",
            "\t1.2s = Fit runtime\n",
            "\t731 features in original data used to generate 731 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 0.33 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 1.41s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 47, Val Rows: 12\n",
            "Fitting 13 L1 models ...\n",
            "Fitting model: KNeighborsUnif ...\n",
            "\t0.5\t = Validation score   (accuracy)\n",
            "\t0.14s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.14s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t1.73s\t = Training   runtime\n",
            "\t0.02s\t = Validation runtime\n",
            "Fitting model: LightGBMXT ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.84s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: LightGBM ...\n",
            "\t0.9167\t = Validation score   (accuracy)\n",
            "\t0.77s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: RandomForestGini ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t1.18s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t1.26s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: CatBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t31.48s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: ExtraTreesGini ...\n",
            "\t0.6667\t = Validation score   (accuracy)\n",
            "\t0.8s\t = Training   runtime\n",
            "\t0.05s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t0.78s\t = Training   runtime\n",
            "\t0.06s\t = Validation runtime\n",
            "Fitting model: XGBoost ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.55s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch ...\n",
            "\t0.5833\t = Validation score   (accuracy)\n",
            "\t0.63s\t = Training   runtime\n",
            "\t0.07s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t2.71s\t = Training   runtime\n",
            "\t0.01s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ...\n",
            "\t1.0\t = Validation score   (accuracy)\n",
            "\t0.48s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 45.89s ... Best model: \"WeightedEnsemble_L2\"\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230221_182524/\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in qual_col:\n",
        "    le = LabelEncoder()\n",
        "    le = le.fit(train_x[i])\n",
        "    train_x[i] = le.transform(train_x[i])\n",
        "    \n",
        "    for label in np.unique(test_x[i]): \n",
        "        if label not in le.classes_: \n",
        "            le.classes_ = np.append(le.classes_, label)\n",
        "    test_x[i] = le.transform(test_x[i])\n",
        "print('Done.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHXZJ4i1EGYY",
        "outputId": "120c3af1-54e0-47f9-84c8-ace65d62da62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(6):\n",
        "  y_preds[i] = y_preds[i].values"
      ],
      "metadata": {
        "id": "shqayDnU4bW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212e0ff8-ff45-422c-bf5a-c28e700fdc63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-d1d88769ee89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_preds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = []\n",
        "cnt = [0 for i in range(6)]\n",
        "test_prd = test_x['LINE']\n",
        "for idx,i in enumerate(test_prd):\n",
        "  if i == 2:\n",
        "    test_pred.append(y_preds[0][cnt[0]])\n",
        "    cnt[0] += 1\n",
        "  elif i == 3:\n",
        "    test_pred.append(y_preds[1][cnt[1]])\n",
        "    cnt[1] += 1\n",
        "  elif i == 4:\n",
        "    test_pred.append(y_preds[2][cnt[2]])\n",
        "    cnt[2] += 1\n",
        "  elif i == 5:\n",
        "    test_pred.append(y_preds[3][cnt[3]])\n",
        "    cnt[3] += 1\n",
        "  elif i == 1:\n",
        "    test_pred.append(y_preds[4][cnt[4]])\n",
        "    cnt[4] += 1\n",
        "  else:\n",
        "    test_pred.append(y_preds[5][cnt[5]])\n",
        "    cnt[5] += 1"
      ],
      "metadata": {
        "id": "O1YdJuDK_eh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test_pred)):\n",
        "  print(test_pred[i])"
      ],
      "metadata": {
        "id": "O4tIJTjbFixX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#save"
      ],
      "metadata": {
        "id": "NuD8ODK9Yl3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['Y_Class'] = test_pred\n",
        "path = 'baselinne.csv'\n",
        "submit.to_csv(path, index = False)"
      ],
      "metadata": {
        "id": "xa3zmTaPAIRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "1d6fd6dc-6bde-4f33-ba2c-4f6d42029ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32mnewEDA.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msubmit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Y_Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'baselinne.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msubmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3464\u001b[0m             \u001b[0msparsify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msparsify\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m             \u001b[0mindex_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3466\u001b[0;31m             \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3467\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimplementing\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0mobject\u001b[0m \u001b[0mimplementing\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mreturned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwriters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlibwriters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mCompressionOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mFilePath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FilePath' from 'pandas._typing' (/usr/local/lib/python3.8/dist-packages/pandas/_typing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC3cglpN5Qvz",
        "outputId": "272346ae-ed27-4d91-9eb7-2e94936a21f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n"
          ]
        }
      ]
    }
  ]
}